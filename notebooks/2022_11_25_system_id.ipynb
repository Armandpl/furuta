{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check if the model is differentiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class QubeDynamics(torch.nn.Module):\n",
    "    \"\"\"Solve equation M qdd + C(q, qd) = tau for qdd.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Gravity\n",
    "        # self.g = Parameter(data=torch.Tensor([9.81]), requires_grad=True)\n",
    "        self.g = torch.tensor([9.81])\n",
    "\n",
    "        # Motor\n",
    "        self.Rm = Parameter(data=torch.Tensor([8.4]), requires_grad=True)\n",
    "\n",
    "        # back-emf constant (V-s/rad)\n",
    "        self.km = Parameter(data=torch.Tensor([0.042]), requires_grad=True)\n",
    "\n",
    "        # Rotary arm\n",
    "        self.Mr = Parameter(data=torch.Tensor([0.095]), requires_grad=True)\n",
    "        self.Lr = Parameter(data=torch.Tensor([0.085]), requires_grad=True)\n",
    "        self.Dr = Parameter(data=torch.Tensor([5e-6]), requires_grad=True)\n",
    "\n",
    "        # Pendulum link\n",
    "        self.Mp = Parameter(data=torch.Tensor([0.024]), requires_grad=True)\n",
    "        self.Lp = Parameter(data=torch.Tensor([0.129]), requires_grad=True)\n",
    "        self.Dp = Parameter(data=torch.Tensor([1e-6]), requires_grad=True)\n",
    "\n",
    "        # Init constants\n",
    "        # self._init_const()\n",
    "\n",
    "    def set_random_params(self):\n",
    "        for p in self.parameters():\n",
    "            p.data = torch.rand_like(p.data)/10 # most params between 0 and 0.1\n",
    "\n",
    "        # except for Rm\n",
    "        self.Rm = Parameter(data=torch.Tensor([5]), requires_grad=True)\n",
    "        \n",
    "        # self._init_const()\n",
    "\n",
    "    def _init_const(self):\n",
    "        # Moments of inertia\n",
    "        Jr = self.Mr * self.Lr ** 2 / 12  # inertia about COM (kg-m^2)\n",
    "        Jp = self.Mp * self.Lp ** 2 / 12  # inertia about COM (kg-m^2)\n",
    "\n",
    "        # Constants for equations of motion\n",
    "        self._c = torch.zeros(5)\n",
    "        self._c[0] = Jr + self.Mp * self.Lr ** 2\n",
    "        self._c[1] = 0.25 * self.Mp * self.Lp ** 2\n",
    "        self._c[2] = 0.5 * self.Mp * self.Lp * self.Lr\n",
    "        self._c[3] = Jp + self._c[1]\n",
    "        self._c[4] = 0.5 * self.Mp * self.Lp * self.g\n",
    "\n",
    "\n",
    "    def forward(self, s_batch, u_batch, dt_batch):\n",
    "        # Unbind the batch tensors to get individual tensors for each input\n",
    "        s_list = torch.unbind(s_batch, dim=0)\n",
    "        u_list = torch.unbind(u_batch, dim=0)\n",
    "        dt_list = torch.unbind(dt_batch, dim=0)\n",
    "\n",
    "        # Create a list to store the next states for each input in the batch\n",
    "        next_state_list = []\n",
    "\n",
    "        # need to re-init each time we update params\n",
    "        self._init_const()\n",
    "\n",
    "        # Loop over the tensors in the batch\n",
    "        for s, u, dt in zip(s_list, u_list, dt_list):\n",
    "            th, al, thd, ald = s\n",
    "            voltage = u[0] * 12\n",
    "\n",
    "\n",
    "            # Define mass matrix M = [[a, b], [b, c]]\n",
    "            a = self._c[0] + self._c[1] * torch.sin(al) ** 2\n",
    "            b = self._c[2] * torch.cos(al)\n",
    "            c = self._c[3]\n",
    "            d = a * c - b * b\n",
    "\n",
    "            # Calculate vector [x, y] = tau - C(q, qd)\n",
    "            trq = self.km * (voltage - self.km * thd) / self.Rm\n",
    "            c0 = self._c[1] * torch.sin(2 * al) * thd * ald \\\n",
    "                - self._c[2] * torch.sin(al) * ald * ald\n",
    "            c1 = -0.5 * self._c[1] * torch.sin(2 * al) * thd * thd \\\n",
    "                + self._c[4] * torch.sin(al)\n",
    "            x = trq - self.Dr * thd - c0\n",
    "            y = -self.Dp * ald - c1\n",
    "\n",
    "            # Compute M^{-1} @ [x, y]\n",
    "            thdd = (c * x - b * y) / d\n",
    "            aldd = (a * y - b * x) / d\n",
    "\n",
    "            next_state = torch.clone(s)\n",
    "            next_state[3] += (dt * aldd)[0]\n",
    "            next_state[2] += (dt * thdd)[0]\n",
    "            next_state[1] += (dt * next_state[3])[0]\n",
    "            next_state[0] += (dt * next_state[2])[0]\n",
    "\n",
    "            next_state_list.append(next_state)\n",
    "\n",
    "        return torch.stack(next_state_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# from furuta_gym.envs.furuta_sim import QubeDynamics as QD\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# baseline = QD()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m QubeDynamics()\n\u001b[0;32m----> 6\u001b[0m state, action, dt, next_state \u001b[39m=\u001b[39m ds[\u001b[39m10\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[39m# run model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m pred_next_state \u001b[39m=\u001b[39m model(state, action, dt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "# from furuta_gym.envs.furuta_sim import QubeDynamics as QD\n",
    "\n",
    "# baseline = QD()\n",
    "model = QubeDynamics()\n",
    "\n",
    "state, action, dt, next_state = ds[10]\n",
    "\n",
    "# run model\n",
    "pred_next_state = model(state, action, dt)\n",
    "\n",
    "print(next_state)\n",
    "print(pred_next_state)\n",
    "# loss = torch.nn.functional.mse_loss(pred_next_state, next_state)\n",
    "# print(loss)\n",
    "\n",
    "# TODO put the state update in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 161/209 [00:02<00:00, 83.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing ep208_20221207-151228.mcap: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:03<00:00, 69.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0778, -0.0716,  4.2630, -4.3147]),\n",
       " tensor([0.7659]),\n",
       " tensor([0.0200]),\n",
       " tensor([  0.3347,  -0.3242,  12.8458, -12.6317]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataset\n",
    "# input is state + action + dt, output is next state\n",
    "import torch\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "import os\n",
    "from furuta_gym.logging.protobuf.pendulum_state_pb2 import PendulumState\n",
    "from mcap_protobuf.reader import read_protobuf_messages\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MCAPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        if isinstance(root_dir, str):\n",
    "            root_dir = Path(root_dir)\n",
    "\n",
    "        # parse the data\n",
    "        # TODO it's gonna load it all in RAM\n",
    "        # + have some duplicates\n",
    "        # but should be ok since this is pretty light < 1MB\n",
    "        self.samples = []\n",
    "        for f in tqdm(os.listdir(root_dir)):\n",
    "            if f.endswith(\".mcap\"):\n",
    "                try:\n",
    "                    self.parse_mcap(root_dir / f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {f}: {e}\")\n",
    "\n",
    "    def parse_mcap(self, pth):\n",
    "        msgs = list(read_protobuf_messages(pth, log_time_order=True))\n",
    "        for i in range(1, len(msgs)-1):\n",
    "            msg = msgs[i-1]\n",
    "            next_msg = msgs[i]\n",
    "\n",
    "            p = msg.proto_msg\n",
    "            state = torch.tensor([p.motor_angle, p.pendulum_angle, \n",
    "                                  p.motor_angle_velocity, p.pendulum_angle_velocity],\n",
    "                                  requires_grad=False,\n",
    "                                  dtype=torch.float32)\n",
    "\n",
    "            next_p = next_msg.proto_msg\n",
    "            next_state = torch.tensor([next_p.motor_angle, next_p.pendulum_angle, \n",
    "                                       next_p.motor_angle_velocity, next_p.pendulum_angle_velocity],\n",
    "                                       requires_grad=False,\n",
    "                                       dtype=torch.float32)\n",
    "\n",
    "            dt = torch.tensor([(next_msg.log_time - msg.log_time).total_seconds()], requires_grad=False)\n",
    "            # dt = torch.tensor([1/50], requires_grad=False)\n",
    "            action = torch.tensor([next_p.corrected_action], requires_grad=False)\n",
    "\n",
    "            sample = (state, action, dt, next_state)\n",
    "            self.samples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "ds = MCAPDataset(\"../data/outul4rm/\")\n",
    "print(len(ds))\n",
    "ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/armand/Dev/furuta/notebooks/wandb/run-20221208_135855-28t48f28</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/armandpl/furuta/runs/28t48f28\" target=\"_blank\">dark-oath-1112</a></strong> to <a href=\"https://wandb.ai/armandpl/furuta\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 169/209 [00:01<00:00, 96.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing ep208_20221207-151228.mcap: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:02<00:00, 83.03it/s]\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dark-oath-1112</strong>: <a href=\"https://wandb.ai/armandpl/furuta/runs/28t48f28\" target=\"_blank\">https://wandb.ai/armandpl/furuta/runs/28t48f28</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221208_135855-28t48f28/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of AsStridedBackward0, is at version 8; expected version 6 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m l \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum( (pred_next_states \u001b[39m-\u001b[39m next_states) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m )\n\u001b[1;32m     66\u001b[0m \u001b[39m# backprop\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m l\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     69\u001b[0m \u001b[39m# update weights\u001b[39;00m\n\u001b[1;32m     70\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/furuta/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/furuta/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of AsStridedBackward0, is at version 8; expected version 6 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "config = {\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 512,\n",
    "    \"start_lr\": 1e-1,\n",
    "    \"end_lr\": 0.25e-3,\n",
    "    \"subset_pct\": 1\n",
    "}\n",
    "with wandb.init(project=\"furuta\", job_type=\"system_id\", config=config) as run:\n",
    "    config = run.config\n",
    "\n",
    "    # setup dataset\n",
    "    ds = MCAPDataset(\"../data/outul4rm/\")\n",
    "\n",
    "    # take a random subset of the data\n",
    "    ds = torch.utils.data.Subset(ds, torch.randperm(len(ds))[:int(len(ds) * config.subset_pct)])\n",
    "\n",
    "    # setup dataloader\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    # setup model\n",
    "    model = QubeDynamics()\n",
    "    model.set_random_params()\n",
    "    model.train()\n",
    "\n",
    "    bn_layer = torch.nn.BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    \n",
    "    wandb.watch(model, log_freq=10, log=\"gradients\")\n",
    "\n",
    "    # setup optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.start_lr)\n",
    "\n",
    "    # setup loss\n",
    "    # loss = torch.nn.MSELoss()\n",
    "\n",
    "    # lr starts at start lr and ends at end lr when epoch = epochs\n",
    "    lambda1 = lambda epoch: config.start_lr + (config.end_lr - config.start_lr) * epoch / config.epochs\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(config.epochs):\n",
    "        for batch in tqdm(dl):\n",
    "            # unpack batch\n",
    "            states, actions, dts, next_states = batch\n",
    "\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # normalize inputs\n",
    "            inputs = torch.cat([states, actions, dts], dim=1)\n",
    "            inputs = bn_layer(inputs)\n",
    "\n",
    "            # re-split inputs into states, actions, dts\n",
    "            states, actions, dts = torch.split(inputs, [4, 1, 1], dim=1)\n",
    "\n",
    "            # run model\n",
    "            # preds = []\n",
    "            # for i in range(state.size()[0]):\n",
    "            #     preds.append(model(state[i], action[i], dt[i]))\n",
    "            pred_next_states = model(states, actions, dts)\n",
    "\n",
    "            # calculate loss (least squares)\n",
    "            l = torch.sum( (pred_next_states - next_states) ** 2 )\n",
    "\n",
    "            # backprop\n",
    "            l.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # build gradient dict\n",
    "            grads = {}\n",
    "            for name, param in model.named_parameters():\n",
    "                grads[f\"gradients/{name}\"] = param.grad\n",
    "\n",
    "            to_log = dict(model.state_dict())\n",
    "            to_log[\"loss\"] = l\n",
    "\n",
    "            # merge to_log and grads\n",
    "            to_log = {**to_log, **grads} \n",
    "            run.log(to_log)\n",
    "        \n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('furuta')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "951a8142a6b321a3a3ccfa39cde286cae4f976f21f3127da7dcf2ae2128da797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
